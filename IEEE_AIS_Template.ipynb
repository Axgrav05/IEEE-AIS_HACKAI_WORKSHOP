{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Welcome to IEEE x AIS Workshop!**\n",
        "\n",
        "In this notebook, we will:\n",
        "1. Create an artificial dataset\n",
        "2. Import and explore the data\n",
        "3. Visualize relationships using graphs\n",
        "4. Preprocess the data\n",
        "5. Train a Machine Learning Model\n",
        "6. Evaluate the model's performance\n"
      ],
      "metadata": {
        "id": "xraI4xpmscvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 1: INSTALL & IMPORT REQUIRED LIBRARIES ---\n",
        "!pip install seaborn scikit-learn"
      ],
      "metadata": {
        "id": "J7mRZLBVtLih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "KbG_fnoit9jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Creating an Artificial Dataset**\n",
        "To demonstrate data science concepts, we generate a synthetic dataset mimicking electronic component failure."
      ],
      "metadata": {
        "id": "sH_h_30RuMBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 2: GENERATE ARTIFICIAL DATASET ---\n",
        "n_samples = 1000\n",
        "np.random.seed(42)\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    \"Voltage_Tolerance\": np.random.normal(5, 0.5, n_samples),\n",
        "    \"Temperature_Cycle\": np.random.randint(0, 500, n_samples),\n",
        "    \"Humidity_Level\": np.random.uniform(10, 90, n_samples),\n",
        "    \"Manufacturing_Defect\": np.random.choice([0, 1], n_samples, p=[0.99, 0.01]),\n",
        "    \"Vibration_Exposure\": np.random.uniform(0, 5, n_samples),\n",
        "    \"Component_Age\": np.random.randint(1, 10, n_samples),\n",
        "    \"Failure_Label\": np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n",
        "})\n",
        "\n",
        "print(\"Sample of Dataset:\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "NqrKNsC-vEBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Exploring the Dataset**\n",
        "Before training a model, we need to understand the dataset."
      ],
      "metadata": {
        "id": "1DLeqFU_wnH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 3: DATA EXPLORATION ---\n",
        "print(\"Dataset Overview:\")\n",
        "print(data.describe())\n",
        "\n",
        "print(\"\\nClass Distribution:\")\n",
        "print(data[\"Failure_Label\"].value_counts())"
      ],
      "metadata": {
        "id": "fY0nkQJdwtP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Visualizing the Data**\n",
        "We'll use seaborn and matplotlib to analyze feature relationships.\n"
      ],
      "metadata": {
        "id": "wILg22Acw2-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 4: DATA VISUALIZATION ---\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "# TODO: Replace ___ with the correct argument for correlation matrix\n",
        "sns.heatmap(___, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# TODO: Replace ___ with the correct argument for pairplot\n",
        "sns.pairplot(___, hue=\"Failure_Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AuF1fzDqw_ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Preprocessing Data**\n",
        "We need to split the data into training and test sets and normalize it."
      ],
      "metadata": {
        "id": "qj5CBvhnxkkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 5: PREPROCESSING DATA ---\n",
        "# TODO: Drop Failure_Label column to create feature matrix\n",
        "X = ___\n",
        "\n",
        "# TODO: Set Failure_Label as the target variable\n",
        "y = ___\n",
        "\n",
        "# Split data into training and test sets\n",
        "# TODO: Use train_test_split function\n",
        "X_train, X_test, y_train, y_test = ___(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "# TODO: Fit and transform the training dat\n",
        "X_train_scaled = ___(X_train)\n",
        "# TODO: Transform the test data\n",
        "X_test_scaled = ___(X_test)"
      ],
      "metadata": {
        "id": "_YF3HpVuxvVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Training a Machine Learning Model**\n",
        "We'll use a Random Forest Classifier to predict failures."
      ],
      "metadata": {
        "id": "p2oVWhO_yWnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 6: MODEL TRAINING ---\n",
        "# TODO: Initialize a RandomForestClassifier model\n",
        "model = ___\n",
        "\n",
        "# TODO: Train the model using training data\n",
        "model.fit(___, ___)"
      ],
      "metadata": {
        "id": "TJirUQdaysl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6: Evaluating the Model**\n",
        "Checking accuracy and performance of the model.\n"
      ],
      "metadata": {
        "id": "mhcO5wa-y3rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 7: MODEL EVALUATION ---\n",
        "# TODO: Predict on the test set\n",
        "y_pred = ___\n",
        "\n",
        "print(\"Model Accuracy:\", ___(y_test, y_pred)) # TODO: Fill correct function\n",
        "print(\"\\nClassification Report:\\n\", ___(y_test, y_pred)) # TODO: Fill correct function"
      ],
      "metadata": {
        "id": "iE5R_Vmjy9c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 7: Visualizing Performance**\n",
        "Confusion matrix visualization to interpret model results."
      ],
      "metadata": {
        "id": "LaFkpw9IzOBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 8: CONFUSION MATRIX ---\n",
        "# TODO: Compute the confusion matrix\n",
        "conf_matrix = ___(y_test, y_pred)\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Reliable\", \"Failure\"], yticklabels=[\"Reliable\", \"Failure\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e0yq8X3TzSX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}